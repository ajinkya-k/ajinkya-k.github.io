[
  {
    "objectID": "funstuff.html",
    "href": "funstuff.html",
    "title": "Ajinkya Kokandakar",
    "section": "",
    "text": "The majority of my free time is spent reading something or listening to something.\n\n\nPodcasts I like\n\nI love podcasts! I discovered the magical world of podcasts sometime in 2017. Haven’t stopped since. My podcast app of choice is Pocket Casts. A sample of some great podcasts:\n\nSeen and the Unseen by Amit Varma: India specific podcast, lots of social science and philosophy. Long form (sometimes &gt; 5hr long) interview/discussion.\nZero by Akshat Rathi: explores the policies, tactics and clean technologies taking us to a future of zero emissions as the world fights climate change. Climate reporter Akshat Rathi talks to the people who are making it happen. Produced by Bloomberg Green.\nEmpire by Anita Anand and William Dalrymple: A great podcast about empires and their history.\nThe Anthropocene Reviewed by the incredible John Green: This is my favorite podcast, by a country mile!\nGrand Tamasha hosted by Milan Vaishnav:  Discusses the latest developments in Indian politics, economics, foreign policy, society, and culture for a global audience\nIdeas of India by Shruti Rajagopalan: Economis Shruti Rajagopalan examines the ideas that will propel India forward, via conversations with researchers and top thinkers in social science and beyond.\n\n\n\nComputing Up with Michael Littman and Dave Ackley: Conversations about computing writ large \nCasual Inference hosted by Lucy D’Agostino McGowan and Ellie Murray: podcast about statistics, causal inference, epidemiology, and a bunch of related things.\nThe Gerrymandering Project by the good folks at FiveThirtyEight and also Dave Wasserman: podcast (plus interactives) about the fascinating complexities of gerrymandering and more importantly self-sorting!\nRachman Review with Gideon Rachman: A podcast about international politics by the Financial Times.\nSway with Kara Swisher: Interview podcast unpacking power and technology\nVolts by David Roberts: A newsletter/podcast about clean energy and politics. (Pretty wonky, which is why I love it!)"
  },
  {
    "objectID": "funstuff.html#podcasts",
    "href": "funstuff.html#podcasts",
    "title": "Ajinkya Kokandakar",
    "section": "",
    "text": "The majority of my free time is spent reading something or listening to something.\n\n\nPodcasts I like\n\nI love podcasts! I discovered the magical world of podcasts sometime in 2017. Haven’t stopped since. My podcast app of choice is Pocket Casts. A sample of some great podcasts:\n\nSeen and the Unseen by Amit Varma: India specific podcast, lots of social science and philosophy. Long form (sometimes &gt; 5hr long) interview/discussion.\nZero by Akshat Rathi: explores the policies, tactics and clean technologies taking us to a future of zero emissions as the world fights climate change. Climate reporter Akshat Rathi talks to the people who are making it happen. Produced by Bloomberg Green.\nEmpire by Anita Anand and William Dalrymple: A great podcast about empires and their history.\nThe Anthropocene Reviewed by the incredible John Green: This is my favorite podcast, by a country mile!\nGrand Tamasha hosted by Milan Vaishnav:  Discusses the latest developments in Indian politics, economics, foreign policy, society, and culture for a global audience\nIdeas of India by Shruti Rajagopalan: Economis Shruti Rajagopalan examines the ideas that will propel India forward, via conversations with researchers and top thinkers in social science and beyond.\n\n\n\nComputing Up with Michael Littman and Dave Ackley: Conversations about computing writ large \nCasual Inference hosted by Lucy D’Agostino McGowan and Ellie Murray: podcast about statistics, causal inference, epidemiology, and a bunch of related things.\nThe Gerrymandering Project by the good folks at FiveThirtyEight and also Dave Wasserman: podcast (plus interactives) about the fascinating complexities of gerrymandering and more importantly self-sorting!\nRachman Review with Gideon Rachman: A podcast about international politics by the Financial Times.\nSway with Kara Swisher: Interview podcast unpacking power and technology\nVolts by David Roberts: A newsletter/podcast about clean energy and politics. (Pretty wonky, which is why I love it!)"
  },
  {
    "objectID": "funstuff.html#books",
    "href": "funstuff.html#books",
    "title": "Ajinkya Kokandakar",
    "section": "Books",
    "text": "Books\nI read a lot, with occasional droughts. My Goodreads profile, which can be found here, is reasonably up to date.\n\nWhere to get books?\n\nLibraries\nI love libraries! I frequently use the University of Wisconsin–Madison libraries and the Middleton Public Library. Both have a vast collection of audiobooks and eBooks that are made available through Libby. If you live in the US, chances are your local library provides eBooks and audiobooks through Libby. The Middleton Public Library also provides audiobooks and eBooks via Hoopla Digital. In fact the local library near you may even offer free access to movies, TV shows and documentaries through Kanopy.\n\n\nBuying Physical Books\nIf you want to buy physical copies, I highly recommend BookFinder. Its a search engine that collates a list of online stores where the book you want is available sorted by price (and categorized by used/new). It is perhaps the best way to find the cheapest copy of a book available for sale online."
  },
  {
    "objectID": "libexp.html",
    "href": "libexp.html",
    "title": "Resources",
    "section": "",
    "text": "The internet is full of amazing explanations for all sorts of concepts. The following link is a running list of explanations that I found very insightful. Each author is tagged in the explanation and all credit belongs to the author cited. I have tried my best to credit every author for a page, but if I have missed someone: (a) I apologize and (b) let me know so I can correct that.\nLibrary of Explanations!"
  },
  {
    "objectID": "libexp.html#library-of-explantations",
    "href": "libexp.html#library-of-explantations",
    "title": "Resources",
    "section": "",
    "text": "The internet is full of amazing explanations for all sorts of concepts. The following link is a running list of explanations that I found very insightful. Each author is tagged in the explanation and all credit belongs to the author cited. I have tried my best to credit every author for a page, but if I have missed someone: (a) I apologize and (b) let me know so I can correct that.\nLibrary of Explanations!"
  },
  {
    "objectID": "libexp.html#explanatory-academic-papers",
    "href": "libexp.html#explanatory-academic-papers",
    "title": "Resources",
    "section": "Explanatory Academic Papers",
    "text": "Explanatory Academic Papers\nBelow is a list of academic papers that explain some methodology or concept. The purpose of this list to collect papers that helped me understand a topic and may not contain the first paper to come up with an idea.\nList of academic papers\nDisclaimer: While visualizations are a good hook for getting people interested in mathematics, they should not be used as the sole learning tool. Do read this essay that talks about the possibilities and limitations of visual explainers."
  },
  {
    "objectID": "libexp.html#textbooks-and-references",
    "href": "libexp.html#textbooks-and-references",
    "title": "Resources",
    "section": "Textbooks and References",
    "text": "Textbooks and References\nThe following is a list of textbooks and references that I have found to be helpful, many of which are freely available.\nFreely available | Full List\nUnderstanding linear models, ANOVA and experimental design is really important for statisticians, especially when doing statistical consulting and the most informative books in this reagard are:\n\nANOVA and Mixed Models by Lukas Meier which is freely available\nApplied Linear Statistical Models, by Kutner, Nachtsheim, Neter and Li, which I think has the most complete and thorough treatment of ANOVA"
  },
  {
    "objectID": "libexp.html#blogs",
    "href": "libexp.html#blogs",
    "title": "Resources",
    "section": "Blogs",
    "text": "Blogs\nStatistics Blogs:\n\nAndrew Heiss\nAlex Hayes\nDan Simpson especially this raucously funny post about gaussian processes.\nAndrew Gelman et al.\nKat Hoffman, notably the SuperLearner explainer, and the series on TMLE\nPrecision Analytics Blog especially this really great introduction to INLA by Kathryn Morrison\nOur Coding Club has fantastic tutorials on many subjects related to R programming and statistics. I personally found the mixed effects models tutorial really useful\nJesse Sadler is a historian whose blog has a number of excellent tutorials related to spatial statistics in R and tools for digitial humanities in general\nNoah Greifer is the author of many matching related packages for R. He has a very good explanation of matching weights on his blog.\nNetflix Technology Blog, especially the series on sequential A/B testing (part 1, part 2)"
  },
  {
    "objectID": "libexp.html#visualizing-measure-theory",
    "href": "libexp.html#visualizing-measure-theory",
    "title": "Resources",
    "section": "Visualizing Measure Theory",
    "text": "Visualizing Measure Theory\nThe level of abstraction provided by measure theory is the reason why it is a powerful and general tool for proving results in mathematics. However, this abstraction also makes it difficult to understand the concepts in measure theory. The following are a few interactive demos I made for myself to understand measure theory. They are incomplete and I have added little exposition if any. I plan to update them sometime in the future.\nConvergence of Random Variables demo Lebesgue Integral demo"
  },
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "Statistical Consulting at UW-Madison",
    "section": "",
    "text": "I work as a Project Assistant in the Statistical Consulting Group which is a part of the Department of Statistics at the University of Wisconsin–Madison. As a consultant, I have worked with more than 20 clients helping them with experimental design, data analysis and conveying statistical results. If you are affiliated with UW-Madison and want statistics-related help for your research project, please visit the group’s website."
  },
  {
    "objectID": "hidden.html",
    "href": "hidden.html",
    "title": "Hidden listings page",
    "section": "",
    "text": "Faster rank(::QRPivoted) function\n\n\n\n\n\n\nJulia\n\n\nLinear Algebra\n\n\n\nBenchmarking implementations of rank functions\n\n\n\n\n\nNov 28, 2024\n\n\nAjinkya Kokandakar\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hidden/2024-11-28-allocations-runtime/index.html",
    "href": "hidden/2024-11-28-allocations-runtime/index.html",
    "title": "Faster rank(::QRPivoted) function",
    "section": "",
    "text": "Why am I writing about this?\nI have recently been experimenting with Julia and fell into computational linear algebra rabbit hole.\n\nusing LinearAlgebra\nusing Chairmarks\nusing PrettyChairmarks\nusing Statistics\nimport LinearAlgebra: rank\nusing TidierPlots\nusing DataFrames\n\n\n\nComputing the rank from a pivoted QR decomposition\nLet’s generate a \\(n \\times n\\) matrix say \\(A\\) with rank \\(r\\), by first generating a \\(n \\times r\\) matrix and then computing its outerproduct. This function was based on a stackoverflow answer here.\n\nn = 100\nr = 20\n\nX = randn(n, r)\n\noutprod(X::Matrix) = X * X'\nA = X * X'\n\nNow let’s compute the pivoted QR factorization of A.\n\nAqr = qr(A, ColumnNorm());\n\n\n\nGenerate matrix of known rank\nThe following function generates a \\(n \\times m\\) matrix of rank \\(r \\leq \\min(n, m)\\). We will use this for benchmarking the implementations.\n\n# Generate an `n × m` matrix of rank `r`\nfunction generatematrix(n::T, m::T, r::T) where {T&lt;:Integer}\n    A = randn(n, r)\n    B = randn(m, r)\n    return A * B'\nend\n\ngeneratematrix (generic function with 1 method)\n\n\n\n\nComputing the rank\nThe current implmentation in the julia repo is as follows:\n\nfunction rank(A::QRPivoted; atol::Real=0, rtol::Real=min(size(A)...) * eps(real(float(one(eltype(A.Q))))) * iszero(atol))\n    m = min(size(A)...)\n    m == 0 && return 0\n    tol = max(atol, rtol*abs(A.R[1,1]))\n    return something(findfirst(i -&gt; abs(A.R[i,i]) &lt;= tol, 1:m), m+1) - 1\nend\n\nrank (generic function with 16 methods)\n\n\nThe following implementation (ranfast) is orders of magnitude faster. See sections below for tests and benchmarks.\n\nfunction rankfast(A::QRPivoted; atol::Real=0, rtol::Real=min(size(A)...) * eps(real(float(one(eltype(A.Q))))) * iszero(atol))\n    m = min(size(A)...)\n    m == 0 && return 0\n    rdiag = diag(getfield(A, :factors))\n    tol = max(atol, rtol*abs(rdiag[1]))\n\n    return something(findfirst(abs.(rdiag) .&lt;= tol), m+1) - 1\nend\n\nrankfast (generic function with 1 method)\n\n\n\nrank(Aqr) == rankfast(Aqr)\n\ntrue\n\n\n\nrankbench = @bs rank($Aqr)\n\nChairmarks.Benchmark: 308 samples with 1 evaluation.\n Range (min … max):   95.250 μs … 14.765 ms  ┊ GC (min … max): 0.00% … 98.80%\n Time  (median):     179.750 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   302.481 μs ±  1.131 ms  ┊ GC (mean ± σ):  0.96% ±  9.66%\n\n       ▇▇▇█▆▄▁                                                  \n  ▄▄▁▆▇███████▇█▆▁▇▆▁▄▁▁▁▄▁▁▄▁▁▄▄▁▄▁▁▁▁▁▄▁▁▁▁▁▁▁▄▁▁▄▄▁▄▄▁▁▁▁▁▄ ▇\n  95.3 μs       Histogram: log(frequency) by time       730 μs &lt;\n\n Memory estimate: 1.68 MiB, allocs estimate: 67.\n\n\n\nrankfastbench = @bs rankfast($Aqr)\n\nChairmarks.Benchmark: 3996 samples with 140 evaluations.\n Range (min … max):   86.007 ns … 42.949 μs  ┊ GC (min … max): 0.00% … 99.40%\n Time  (median):     111.307 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   161.788 ns ±  1.116 μs  ┊ GC (mean ± σ):  0.20% ±  4.41%\n\n   ▄  ▅▅▆█▆▃     ▃▃ ▂▁▁                                         \n  ▆█▅▇██████▅▄▄▆█████████▇▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▂▂▂▁▂ ▄\n  86 ns           Histogram: frequency by time          224 ns &lt;\n\n Memory estimate: 1.05 KiB, allocs estimate: 6.\n\n\n\n\nVarying the rank\nNow we generate matrices of size \\(1000 \\times 100\\), with ranks increasing from 1 to 100, to compare the two implementations.\n\nn = 1000; m = 100\n\n\nbenchs = Vector{Chairmarks.Benchmark}()\nbenchf = Vector{Chairmarks.Benchmark}()\n\n\nfor r in 1:min(n,m)\n  _qrA = qr(generatematrix(n, m, r), ColumnNorm())\n  _br = @be rank($_qrA)\n  _brf = @be rankfast($_qrA)\n\n  if (rank(_qrA) != rankfast(_qrA)) \n    println(\"uh oh rank didnt match\") \n  else\n    print(\".\")\n  end\n\n  push!(benchs, _br)\n  push!(benchf, _brf)\nend\n\n....................................................................................................\n\n\n\nmedslow = [median(bnc) for bnc in benchs]\nmedfast = [median(bnc) for bnc in benchf]\n\n\nxs = collect(1:min(m,n))\n\ndf = DataFrame(\n  rank = xs,\n  median_time = [b.time for b in medslow],\n  median_time_fast = [b.time for b in medfast],\n  medianbytes = [b.bytes for b in medslow],\n  medianbytes_fast = [b.bytes for b in medfast])\n\nggplot(df) +\n  geom_line(\n    @aes(x = rank, y = median_time), color = \"red\"\n  ) +\n  geom_line(\n    @aes(x = rank, y = median_time_fast), color = \"blue\"\n  ) +\n  labs(\n    x = \"rank\", \n    y = \"runtime (in seconds, log scale)\", \n    title = \"runtime of current (red) and faster (blue) method for $n x $m matrix\"\n  ) +\n  scale_y_log10()\n\n\n\n\n\nxs = collect(1:min(m,n))\n\nggplot(df) +\n  geom_line(\n    @aes(x = rank, y = medianbytes), color = \"red\"\n  ) +\n  geom_line(\n    @aes(x = rank, y = medianbytes_fast), color = \"blue\"\n  ) +\n  labs(x = \"rank\", \n  y = \"memory allocated (in bytes, log scale)\", title = \"Median memory allocated of current (red) and faster (blue) method for $n x $m matrix\") +\n  scale_y_log10()\n\n\n\n\n\n\nWhy is the current implementaiton so slow?\nIt’s slow because it calls Aqr.R for every diagonal entry, which leads to computing the upper triangular matrix each time! If min(n, m) is large, that can mean recreating this matrix a lot of times! The code below demonstrates this.\n\n\nA = generatematrix(10000, 100, 80);\nAqr = qr(A, ColumnNorm());\n\n\n@bs for i in 1:min(size(A)...) Aqr.R[i,i] end\n\nChairmarks.Benchmark: 37 samples with 1 evaluation.\n Range (min … max):  551.416 μs … 95.373 ms  ┊ GC (min … max):  0.00% … 98.71%\n Time  (median):     807.709 μs              ┊ GC (median):     0.00%\n Time  (mean ± σ):     4.401 ms ± 15.885 ms  ┊ GC (mean ± σ):  10.21% ± 29.74%\n\n  █                                                             \n  █▁▁▅▁▁▁▁▅▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅ ▁\n  551 μs        Histogram: log(frequency) by time      95.4 ms &lt;\n\n Memory estimate: 7.64 MiB, allocs estimate: 503.\n\n\n\n@bs for i in 1:min(size(A)...) Aqr.factors[i,i] end\n\nChairmarks.Benchmark: 3890 samples with 5 evaluations.\n Range (min … max):  4.367 μs …   9.875 μs  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     4.767 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   4.900 μs ± 452.414 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n        ▁▄█▅▄▄▄▁                                               \n  ▂▃▂▂▂▄█████████▆▆▄▄▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▂▁▂▂▁▁▁▁▁▁ ▂\n  4.37 μs         Histogram: frequency by time        6.38 μs &lt;\n\n Memory estimate: 4.77 KiB, allocs estimate: 203.\n\n\n\n@bs Aqr.R[1,1]\n\nChairmarks.Benchmark: 2278 samples with 3 evaluations.\n Range (min … max):   3.889 μs …   3.216 ms  ┊ GC (min … max): 0.00% … 99.10%\n Time  (median):      8.056 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   13.169 μs ± 101.508 μs  ┊ GC (mean ± σ):  0.22% ±  4.61%\n\n              ▄█▅▆▆▂▂▁  ▁                                       \n  ▇▃▄▃▃▂▃▃▂▂▂▆████████████▆▅▅▄▅▃▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▁▁▁▂ ▃\n  3.89 μs         Histogram: frequency by time         18.6 μs &lt;\n\n Memory estimate: 78.26 KiB, allocs estimate: 5.\n\n\n\n@bs Aqr.factors[1,1]\n\nChairmarks.Benchmark: 3080 samples with 1127 evaluations.\n Range (min … max):  23.883 ns … 85.478 ns  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     26.619 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   27.001 ns ±  2.016 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n             ▁▅▅█▄▄▇▁▄                                         \n  ▁▁▂▂▂▂▂▄▅▇▇██████████▇▆▄▅▄▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁ ▃\n  23.9 ns         Histogram: frequency by time        33.5 ns &lt;\n\n Memory estimate: 16.099378881987576 bytes, allocs estimate: 1.\n\n\n\n\nAlloc can get bad for large matrices!\nIf the matrix is large (even if the rank is small), the current method allocates too much! See example below.\n\nMqr = qr(generatematrix(1000, 100, 50), ColumnNorm());\n\nThe current implementation allocated ~400 megabytes and takes on the order of 100 ms to run!\n\n@bs rank($Mqr)\n\nChairmarks.Benchmark: 181 samples with 1 evaluation.\n Range (min … max):  201.125 μs …   4.482 ms  ┊ GC (min … max): 0.00% … 86.52%\n Time  (median):     369.584 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   459.190 μs ± 438.739 μs  ┊ GC (mean ± σ):  9.92% ± 22.46%\n\n  █                                                              \n  ██▆▄▆▅▅▆▅▅▄▄▃▄▂▃▃▁▂▃▁▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂ ▂\n  201 μs           Histogram: frequency by time         2.32 ms &lt;\n\n Memory estimate: 3.97 MiB, allocs estimate: 157.\n\n\nThe faster method only allocates ~10 kilobytes and the runtime is in μs!!\n\n@bs rankfast($Mqr)\n\nChairmarks.Benchmark: 3584 samples with 129 evaluations.\n Range (min … max):   87.209 ns … 147.404 μs  ┊ GC (min … max): 0.00% … 99.79%\n Time  (median):     115.953 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   196.082 ns ±   2.558 μs  ┊ GC (mean ± σ):  0.19% ±  4.36%\n\n       █▄▃▂▆▇▅▁                                                  \n  ▂▅▄▄▆████████▇▄▂▃▂▂▂▃▄▅▆▆▇▇█▆▇▆▇▆▆▅▆▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁ ▃\n  87.2 ns          Histogram: frequency by time          215 ns &lt;\n\n Memory estimate: 1.05 KiB, allocs estimate: 6.\n\n\n\n\n\n\nCitationBibTeX citation:@online{kokandakar2024,\n  author = {Kokandakar, Ajinkya},\n  title = {Faster {`rank(::QRPivoted)`} Function},\n  date = {2024-11-28},\n  url = {https://ajinkya-k.github.io/posts/2024-18-24-allocations-runtime/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKokandakar, Ajinkya. 2024. “Faster `Rank(::QRPivoted)`\nFunction.” November 28, 2024. https://ajinkya-k.github.io/posts/2024-18-24-allocations-runtime/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ajinkya Kokandakar",
    "section": "",
    "text": "CV\n  \n  \n    \n     GitHub\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Google Scholar\n  \n  \n     arXiv\n  \n  \n     ORCiD\n  \n\n  \n  \n\n\nPh.D. Candidate in Statistics | Statistical Consultant The University of Wisconsin–Madison ajinkya@stat.wisc.edu\nCurrent: I am working as a Data Science Intern (part-time) at Mathematica Inc. and as a Statistical Consultant (also part-time) on campus at UW–Madison.\nResearch: I am a Ph.D. candidate in Statistics at the UW-Madison advised by Dr. Sameer Deshpande, pursuing research in causal inference and Bayesian methods. Currently, my work focuses on heterogeneous treatment effect estimation, and causal inference when the exposure is not very well defined.\nConsulting: I work as a consultant in the Statistical Consulting Group at UW-Madison (formerly the CALS Consulting Lab), assisting clients from life sciences with experimental design and data analysis.\nBackground: Prior to joining UW—Madison, I graduated with a MS in Economics and Computation at Duke University (2020). I got my undergraduate dual-degree in Computer Science and Economics from Birla Institute of Technology and Science, Pilani in India."
  },
  {
    "objectID": "index.html#ajinkya-kokandakar",
    "href": "index.html#ajinkya-kokandakar",
    "title": "Ajinkya Kokandakar",
    "section": "",
    "text": "Ph.D. Candidate in Statistics | Statistical Consultant The University of Wisconsin–Madison ajinkya@stat.wisc.edu\nCurrent: I am working as a Data Science Intern (part-time) at Mathematica Inc. and as a Statistical Consultant (also part-time) on campus at UW–Madison.\nResearch: I am a Ph.D. candidate in Statistics at the UW-Madison advised by Dr. Sameer Deshpande, pursuing research in causal inference and Bayesian methods. Currently, my work focuses on heterogeneous treatment effect estimation, and causal inference when the exposure is not very well defined.\nConsulting: I work as a consultant in the Statistical Consulting Group at UW-Madison (formerly the CALS Consulting Lab), assisting clients from life sciences with experimental design and data analysis.\nBackground: Prior to joining UW—Madison, I graduated with a MS in Economics and Computation at Duke University (2020). I got my undergraduate dual-degree in Computer Science and Economics from Birla Institute of Technology and Science, Pilani in India."
  },
  {
    "objectID": "index.html#preprints-and-working-projects",
    "href": "index.html#preprints-and-working-projects",
    "title": "Ajinkya Kokandakar",
    "section": "Preprints and Working Projects",
    "text": "Preprints and Working Projects\nAdolescent sports participation and health in early adulthood: An observational study (2024+)  Ajinkya H. Kokandakar, Yuzhou Lin, Steven Jin, Jordan Weiss, Amanda R. Rabinowitz, Reuben A. Buford May, Dylan Small, Sameer K. Deshpande   [preprint] [code]"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Ajinkya Kokandakar",
    "section": "Publications",
    "text": "Publications\nEliot J. Kim, Tracey Holloway, Ajinkya H. Kokandakar, Monica Harkey, Stephanie Elkins, Daniel L. Goldberg, Colleen Heck, (2024) A Comparison of Regression Methods for Inferring Near-Surface NO2 with Satellite Data  Journal of Geophysical Research: Atmospheres, 129, e2024JD040906  [paper] [preprint]\nAjinkya H. Kokandakar, Yuzhou Lin, Steven Jin, Jordan Weiss, Amanda R. Rabinowitz, Reuben A. Buford May, Sameer K. Deshpande, Dylan Small, (2024). “Pre-analysis protocol for an observational study on the effects of adolescent sports participation on health in early adulthood.” Observational Studies 10(1), 11-35  [paper] [preprint] [code]\nAjinkya H. Kokandakar, Hyunseung Kang, Sameer K. Deshpande, (2023). “Bayesian causal forests and the 2022 ACIC Data Challenge: scalability and sensitivity.” Observational Studies, 9(3), 29-41. [paper] [preprint] [code]\nJagat Sesh Challa, Poonam Goyal, Ajinkya Kokandakar, Dhananjay Mantri, Pranet Verma, Sundar Balasubramaniam & Navneet Goyal (2022). “Anytime clustering of data streams while handling noise and concept drift.” Journal of Experimental & Theoretical Artificial Intelligence, 34(3), 399-429.  [paper]"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "Ajinkya Kokandakar",
    "section": "Work Experience",
    "text": "Work Experience\nMathematica Inc. Data Science Intern (June 2024 - August 2024 full-time, part-time since Aug 2024)\n\nDeveloping and using Bayesian non-parametric methods for subgroup analysis in a large scale healthcare impact evaluation\n\nUniversity of Wisconsin – Madison Project Assistant, Statistical Consulting Group (Aug 2023 - May 2024)\n\nAssisted more than 15 clients (graduate students and postdocs) from the College of Agriculture and Life Sciences with experimental design and analysis data obtained from experiments\n\nInfosys Ltd  Specialist Programmer (July 2017 - May 2018)\n\nDesigned and developed the telemetry and data analytics module for the company’s internal learning platform"
  },
  {
    "objectID": "index.html#research-experience",
    "href": "index.html#research-experience",
    "title": "Ajinkya Kokandakar",
    "section": "Research Experience",
    "text": "Research Experience\nUniversity of Wisconsin – Madison Research Assistant\n\nDepartment of Statistics, Advisors: Prof Sameer Deshpande and Prof Keith Levin (May 2022 – Aug 2022)\nDepartment of Biostatistics and Medical Informatics, Advisors: Prof Menggang Yu and Prof Guanhua Chen (June 2020 – Dec 2021)\n\nDuke University Research Assistant\n\nThe Fuqua School of Business Advisor: Prof Giuseppe Lopomo (June 2019 - May 2020)\nDepartment of Economics Advisor: Prof Matt Masten (June 2019 - May 2020)\nDepartment of Economics Advisor: Prof Arjada Bardhi (Jan 2019 - May 2019)\n\nBirla Institute of Technology and Science, Pilani\n\nUndergraduate Scholar, Advisor: Prof S. Balasubramaniam, ADAPT Lab, BITS Pilani (Jan 2017 - Dec 2017)"
  },
  {
    "objectID": "index.html#teaching-experience",
    "href": "index.html#teaching-experience",
    "title": "Ajinkya Kokandakar",
    "section": "Teaching Experience",
    "text": "Teaching Experience\nUniversity of Wisconsin – Madison Teaching Assistant, Department of Statistics\n\nSTAT 451: Introduction to Machine Learning and Statistical Pattern Classification (Fall 2023)\nSTAT 240: Data Science Modeling I (Fall 2022)\nSTAT 371: Introductory Applied Statistics for the Life Sciences (Spring 2022)\n\nDuke University Teaching Assistant, Department of Computer Science\n\nCOMPSCI 370: Introduction to Artificial Intelligence (Spring 2020)\nCOMPSCI 201: Algorithms and Data Structures (Spring 2019)\n\nBirla Institute of Technology and Science, Pilani Undergraduate Teaching Assistant\n\nECON F211: Principles of Economics\nECON F212: Fundamentals of Finance and Accounting\nECON F412: Securities Analysis and Portfolio Management\nCS F211: Data Structures and Algorithms"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "No matching items"
  }
]