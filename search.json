[
  {
    "objectID": "funstuff.html",
    "href": "funstuff.html",
    "title": "Ajinkya Kokandakar",
    "section": "",
    "text": "The majority of my free time is spent reading something or listening to something.\n\n\nPodcasts I like\n\nI love podcasts! I discovered the magical world of podcasts sometime in 2017. Haven’t stopped since. My podcast app of choice is Pocket Casts. A sample of some great podcasts:\n\nSeen and the Unseen by Amit Varma: India specific podcast, lots of social science and philosophy. Long form (sometimes &gt; 5hr long) interview/discussion.\nZero by Akshat Rathi: explores the policies, tactics and clean technologies taking us to a future of zero emissions as the world fights climate change. Climate reporter Akshat Rathi talks to the people who are making it happen. Produced by Bloomberg Green.\nEmpire by Anita Anand and William Dalrymple: A great podcast about empires and their history.\nThe Anthropocene Reviewed by the incredible John Green: This is my favorite podcast, by a country mile!\nGrand Tamasha hosted by Milan Vaishnav:  Discusses the latest developments in Indian politics, economics, foreign policy, society, and culture for a global audience\nIdeas of India by Shruti Rajagopalan: Economis Shruti Rajagopalan examines the ideas that will propel India forward, via conversations with researchers and top thinkers in social science and beyond.\n\n\n\nComputing Up with Michael Littman and Dave Ackley: Conversations about computing writ large \nCasual Inference hosted by Lucy D’Agostino McGowan and Ellie Murray: podcast about statistics, causal inference, epidemiology, and a bunch of related things.\nThe Gerrymandering Project by the good folks at FiveThirtyEight and also Dave Wasserman: podcast (plus interactives) about the fascinating complexities of gerrymandering and more importantly self-sorting!\nRachman Review with Gideon Rachman: A podcast about international politics by the Financial Times.\nSway with Kara Swisher: Interview podcast unpacking power and technology\nVolts by David Roberts: A newsletter/podcast about clean energy and politics. (Pretty wonky, which is why I love it!)"
  },
  {
    "objectID": "funstuff.html#podcasts",
    "href": "funstuff.html#podcasts",
    "title": "Ajinkya Kokandakar",
    "section": "",
    "text": "The majority of my free time is spent reading something or listening to something.\n\n\nPodcasts I like\n\nI love podcasts! I discovered the magical world of podcasts sometime in 2017. Haven’t stopped since. My podcast app of choice is Pocket Casts. A sample of some great podcasts:\n\nSeen and the Unseen by Amit Varma: India specific podcast, lots of social science and philosophy. Long form (sometimes &gt; 5hr long) interview/discussion.\nZero by Akshat Rathi: explores the policies, tactics and clean technologies taking us to a future of zero emissions as the world fights climate change. Climate reporter Akshat Rathi talks to the people who are making it happen. Produced by Bloomberg Green.\nEmpire by Anita Anand and William Dalrymple: A great podcast about empires and their history.\nThe Anthropocene Reviewed by the incredible John Green: This is my favorite podcast, by a country mile!\nGrand Tamasha hosted by Milan Vaishnav:  Discusses the latest developments in Indian politics, economics, foreign policy, society, and culture for a global audience\nIdeas of India by Shruti Rajagopalan: Economis Shruti Rajagopalan examines the ideas that will propel India forward, via conversations with researchers and top thinkers in social science and beyond.\n\n\n\nComputing Up with Michael Littman and Dave Ackley: Conversations about computing writ large \nCasual Inference hosted by Lucy D’Agostino McGowan and Ellie Murray: podcast about statistics, causal inference, epidemiology, and a bunch of related things.\nThe Gerrymandering Project by the good folks at FiveThirtyEight and also Dave Wasserman: podcast (plus interactives) about the fascinating complexities of gerrymandering and more importantly self-sorting!\nRachman Review with Gideon Rachman: A podcast about international politics by the Financial Times.\nSway with Kara Swisher: Interview podcast unpacking power and technology\nVolts by David Roberts: A newsletter/podcast about clean energy and politics. (Pretty wonky, which is why I love it!)"
  },
  {
    "objectID": "funstuff.html#books",
    "href": "funstuff.html#books",
    "title": "Ajinkya Kokandakar",
    "section": "Books",
    "text": "Books\nI read a lot, with occasional droughts. My Goodreads profile, which can be found here, is reasonably up to date.\n\nWhere to get books?\n\nLibraries\nI love libraries! I frequently use the University of Wisconsin–Madison libraries and the Middleton Public Library. Both have a vast collection of audiobooks and eBooks that are made available through Libby. If you live in the US, chances are your local library provides eBooks and audiobooks through Libby. The Middleton Public Library also provides audiobooks and eBooks via Hoopla Digital. In fact the local library near you may even offer free access to movies, TV shows and documentaries through Kanopy.\n\n\nBuying Physical Books\nIf you want to buy physical copies, I highly recommend BookFinder. Its a search engine that collates a list of online stores where the book you want is available sorted by price (and categorized by used/new). It is perhaps the best way to find the cheapest copy of a book available for sale online."
  },
  {
    "objectID": "libexp.html",
    "href": "libexp.html",
    "title": "Resources",
    "section": "",
    "text": "The internet is full of amazing explanations for all sorts of concepts. The following link is a running list of explanations that I found very insightful. Each author is tagged in the explanation and all credit belongs to the author cited. I have tried my best to credit every author for a page, but if I have missed someone: (a) I apologize and (b) let me know so I can correct that.\nLibrary of Explanations!"
  },
  {
    "objectID": "libexp.html#library-of-explantations",
    "href": "libexp.html#library-of-explantations",
    "title": "Resources",
    "section": "",
    "text": "The internet is full of amazing explanations for all sorts of concepts. The following link is a running list of explanations that I found very insightful. Each author is tagged in the explanation and all credit belongs to the author cited. I have tried my best to credit every author for a page, but if I have missed someone: (a) I apologize and (b) let me know so I can correct that.\nLibrary of Explanations!"
  },
  {
    "objectID": "libexp.html#explanatory-academic-papers",
    "href": "libexp.html#explanatory-academic-papers",
    "title": "Resources",
    "section": "Explanatory Academic Papers",
    "text": "Explanatory Academic Papers\nBelow is a list of academic papers that explain some methodology or concept. The purpose of this list to collect papers that helped me understand a topic and may not contain the first paper to come up with an idea.\nList of academic papers\nDisclaimer: While visualizations are a good hook for getting people interested in mathematics, they should not be used as the sole learning tool. Do read this essay that talks about the possibilities and limitations of visual explainers."
  },
  {
    "objectID": "libexp.html#textbooks-and-references",
    "href": "libexp.html#textbooks-and-references",
    "title": "Resources",
    "section": "Textbooks and References",
    "text": "Textbooks and References\nThe following is a list of textbooks and references that I have found to be helpful, many of which are freely available.\nFreely available | Full List\nUnderstanding linear models, ANOVA and experimental design is really important for statisticians, especially when doing statistical consulting and the most informative books in this reagard are:\n\nANOVA and Mixed Models by Lukas Meier which is freely available\nApplied Linear Statistical Models, by Kutner, Nachtsheim, Neter and Li, which I think has the most complete and thorough treatment of ANOVA"
  },
  {
    "objectID": "libexp.html#blogs",
    "href": "libexp.html#blogs",
    "title": "Resources",
    "section": "Blogs",
    "text": "Blogs\nStatistics Blogs:\n\nAndrew Heiss\nAlex Hayes\nDan Simpson especially this raucously funny post about gaussian processes.\nAndrew Gelman et al.\nKat Hoffman, notably the SuperLearner explainer, and the series on TMLE\nPrecision Analytics Blog especially this really great introduction to INLA by Kathryn Morrison\nOur Coding Club has fantastic tutorials on many subjects related to R programming and statistics. I personally found the mixed effects models tutorial really useful\nJesse Sadler is a historian whose blog has a number of excellent tutorials related to spatial statistics in R and tools for digitial humanities in general\nNoah Greifer is the author of many matching related packages for R. He has a very good explanation of matching weights on his blog.\nNetflix Technology Blog, especially the series on sequential A/B testing (part 1, part 2)"
  },
  {
    "objectID": "libexp.html#visualizing-measure-theory",
    "href": "libexp.html#visualizing-measure-theory",
    "title": "Resources",
    "section": "Visualizing Measure Theory",
    "text": "Visualizing Measure Theory\nThe level of abstraction provided by measure theory is the reason why it is a powerful and general tool for proving results in mathematics. However, this abstraction also makes it difficult to understand the concepts in measure theory. The following are a few interactive demos I made for myself to understand measure theory. They are incomplete and I have added little exposition if any. I plan to update them sometime in the future.\nConvergence of Random Variables demo Lebesgue Integral demo"
  },
  {
    "objectID": "posts/nb-pois/index.html",
    "href": "posts/nb-pois/index.html",
    "title": "Poisson as a Limit of the Negative Binomial Distribution",
    "section": "",
    "text": "John D. Cook has a great sequence of posts1 about the negative binomial distribution. The standard form of the negative binomial distribution has two parameters, \\(r\\) and \\(p\\), and has a probability mass function (pmf) given by: \\[\n  f(y; r, p) = \\binom{r + y - 1}{y} \\times (1 - p)^{y} \\times p^{r}\n\\]\nThis distribution has mean \\(\\mu = \\frac{r(1-p)}{p}\\) and variance \\(\\sigma^2 = \\frac{r(1-p)}{p^{2}}\\). Although \\(r\\) is sometimes interpreted as the “number of successes” that must be reached, the distribution can be generalized to noninteger positive values of \\(r\\) by expressing the binomial coefficient in terms of the \\(\\Gamma\\) function. To see this, note that for positive-integer values of \\(r\\): \\[\n  \\binom{r + y - 1}{y} = \\frac{(r + y - 1)!}{(r - 1)! \\cdot y!} .\n\\] Since the \\(\\Gamma\\) function is the generalization of the factorial function, we can extend the binomial distribution to noninteger positive \\(r\\) by writing: \\[\n  f(y;\\ r, p) = \\frac{\\Gamma(r + y)}{\\Gamma(r) \\cdot y!} \\times (1 - p)^{y} \\times p^{r} .\n\\]\nIn his notes, John D. Cook mentions the following result:\nProposition: If \\(r \\rightarrow \\infty\\) and \\(p \\rightarrow 1\\) as \\(\\mu\\) stays constant, \\(f(y; r, p)\\) converges to a Poisson probability mass function with mean \\(\\mu\\).\nLet’s prove this result. But first, we need to be more careful when writing the proposition; more specifically we need to be clear about what it means for \\(r \\rightarrow \\infty\\) and \\(p \\rightarrow 1\\) while \\(\\mu\\) is constant. To do so, note that the parameter \\(p = \\frac{r}{\\mu + r}\\). Now if we fix \\(\\mu\\), as \\(r \\rightarrow \\infty\\) we have \\(p \\rightarrow 1\\). We can now rewrite the pmf in terms of the parameters \\(r\\) and \\(\\mu\\) by slightly overloading the notation for the pmf as: \\[\n  f(y; r, \\mu) := f\\left(y;\\ r, \\frac{r}{r + \\mu} \\right) = \\frac{\\Gamma(r + y)}{\\Gamma(r) \\cdot y!} \\times \\frac{r^r \\cdot \\mu^y}{(r + \\mu)^{y + r}}\n\\]\nWe can now formally write the result we want to prove as follows:\n\n\n\n\n\n\nProposition\n\n\n\nFor a constant \\(\\mu\\) as \\(r \\rightarrow \\infty\\), \\(f(y; r, \\mu)\\) converges to a Poisson probability mass function with mean \\(\\mu\\), that is: \\[\n  \\lim_{r \\rightarrow \\infty} f(y; r, \\mu) = \\frac{e^{-\\mu} \\mu^y}{y!}\n\\]\n\n\nProof: Starting with the reparameterized pmf, multiply and divide by \\(r^{y}\\) and rearrange: \\[\n  f(y; r, \\mu) = \\frac{\\Gamma(r + y)}{\\Gamma(r)\\cdot r^{y}} \\times \\left(\\frac{r}{r + \\mu}\\right)^{y + r} \\frac{\\mu^y}{y!} .\n\\] Next, since \\(y\\) is a non-negative integer we can use the definition of the \\(\\Gamma\\)-function to write: \\(\\Gamma(r + y) = \\Gamma(r) \\times \\prod_{j = 1}^{y} (r + j)\\). Plugging this and again re-arranging terms we get: \\[\n  f(y; r, \\mu) = \\prod_{j=1}^{y} \\left(1 + \\frac{j}{r}\\right) \\times \\left(\\frac{1}{1+ \\mu / r}\\right)^{y} \\times \\left(\\frac{1}{1+ \\mu / r}\\right)^{r} \\times  \\frac{\\mu^y}{y!} .\n\\] Now note that \\(\\frac{j}{r} \\rightarrow 0\\) and \\(\\left(\\frac{1}{1+ \\mu / r}\\right)^{y} \\rightarrow 1\\) as \\(r \\rightarrow \\infty\\), so we have: \\[\n  \\lim_{r \\rightarrow \\infty} f(y; r, \\mu)\\ =\\ \\frac{\\mu^y}{y!} \\times \\lim_{r \\rightarrow \\infty} \\left(\\frac{1}{1+ \\mu / r}\\right)^{r} .\n\\] The last bit is easy once we note that: \\[\n\\begin{aligned}\n  \\lim_{r \\rightarrow \\infty} \\left(\\frac{1}{1+ \\mu / r}\\right)^{r}\\ &=\\\n      \\lim_{r \\rightarrow \\infty} \\left(1 + \\frac{\\mu}{r}\\right)^{-r}\\\\\n      &=\\ \\left[\\lim_{r \\rightarrow \\infty} \\left(1 + \\frac{\\mu}{r}\\right)^{r/\\mu}\\right]^{-\\mu}\\\\\n      &=\\ e^{-\\mu} .\n\\end{aligned}\n\\]\nPutting all of this together, we have: \\[\n  \\lim_{r \\rightarrow \\infty} f(y; r, \\mu)\\ =\\ \\frac{e^{-\\mu} \\cdot \\mu^y}{y!}.\n\\]\nFigure 1 shows the negative binomial pmf (in black) with increasing \\(r\\), and the Poisson pmf in blue. We can clearly see the negative binomial pmf resembles the poisson more and more as \\(r\\) increases. The animation was created using the excellent Julia packages Makie.jl and AlgebraOfGraphics.jl, see Danisch and Krumbiegel (2021).\n\n\n\n\n\n\nFigure 1: Negative binomial pmf (black) approaches the Poisson pmf (blue) as \\(r\\) increases.\n\n\n\n\n\nThanks to Steven Moen, Margaret Turner, and Sam Ozminkowski for suggestions and edits."
  },
  {
    "objectID": "posts/nb-pois/index.html#introduction",
    "href": "posts/nb-pois/index.html#introduction",
    "title": "Poisson as a Limit of the Negative Binomial Distribution",
    "section": "",
    "text": "John D. Cook has a great sequence of posts1 about the negative binomial distribution. The standard form of the negative binomial distribution has two parameters, \\(r\\) and \\(p\\), and has a probability mass function (pmf) given by: \\[\n  f(y; r, p) = \\binom{r + y - 1}{y} \\times (1 - p)^{y} \\times p^{r}\n\\]\nThis distribution has mean \\(\\mu = \\frac{r(1-p)}{p}\\) and variance \\(\\sigma^2 = \\frac{r(1-p)}{p^{2}}\\). Although \\(r\\) is sometimes interpreted as the “number of successes” that must be reached, the distribution can be generalized to noninteger positive values of \\(r\\) by expressing the binomial coefficient in terms of the \\(\\Gamma\\) function. To see this, note that for positive-integer values of \\(r\\): \\[\n  \\binom{r + y - 1}{y} = \\frac{(r + y - 1)!}{(r - 1)! \\cdot y!} .\n\\] Since the \\(\\Gamma\\) function is the generalization of the factorial function, we can extend the binomial distribution to noninteger positive \\(r\\) by writing: \\[\n  f(y;\\ r, p) = \\frac{\\Gamma(r + y)}{\\Gamma(r) \\cdot y!} \\times (1 - p)^{y} \\times p^{r} .\n\\]\nIn his notes, John D. Cook mentions the following result:\nProposition: If \\(r \\rightarrow \\infty\\) and \\(p \\rightarrow 1\\) as \\(\\mu\\) stays constant, \\(f(y; r, p)\\) converges to a Poisson probability mass function with mean \\(\\mu\\).\nLet’s prove this result. But first, we need to be more careful when writing the proposition; more specifically we need to be clear about what it means for \\(r \\rightarrow \\infty\\) and \\(p \\rightarrow 1\\) while \\(\\mu\\) is constant. To do so, note that the parameter \\(p = \\frac{r}{\\mu + r}\\). Now if we fix \\(\\mu\\), as \\(r \\rightarrow \\infty\\) we have \\(p \\rightarrow 1\\). We can now rewrite the pmf in terms of the parameters \\(r\\) and \\(\\mu\\) by slightly overloading the notation for the pmf as: \\[\n  f(y; r, \\mu) := f\\left(y;\\ r, \\frac{r}{r + \\mu} \\right) = \\frac{\\Gamma(r + y)}{\\Gamma(r) \\cdot y!} \\times \\frac{r^r \\cdot \\mu^y}{(r + \\mu)^{y + r}}\n\\]\nWe can now formally write the result we want to prove as follows:\n\n\n\n\n\n\nProposition\n\n\n\nFor a constant \\(\\mu\\) as \\(r \\rightarrow \\infty\\), \\(f(y; r, \\mu)\\) converges to a Poisson probability mass function with mean \\(\\mu\\), that is: \\[\n  \\lim_{r \\rightarrow \\infty} f(y; r, \\mu) = \\frac{e^{-\\mu} \\mu^y}{y!}\n\\]\n\n\nProof: Starting with the reparameterized pmf, multiply and divide by \\(r^{y}\\) and rearrange: \\[\n  f(y; r, \\mu) = \\frac{\\Gamma(r + y)}{\\Gamma(r)\\cdot r^{y}} \\times \\left(\\frac{r}{r + \\mu}\\right)^{y + r} \\frac{\\mu^y}{y!} .\n\\] Next, since \\(y\\) is a non-negative integer we can use the definition of the \\(\\Gamma\\)-function to write: \\(\\Gamma(r + y) = \\Gamma(r) \\times \\prod_{j = 1}^{y} (r + j)\\). Plugging this and again re-arranging terms we get: \\[\n  f(y; r, \\mu) = \\prod_{j=1}^{y} \\left(1 + \\frac{j}{r}\\right) \\times \\left(\\frac{1}{1+ \\mu / r}\\right)^{y} \\times \\left(\\frac{1}{1+ \\mu / r}\\right)^{r} \\times  \\frac{\\mu^y}{y!} .\n\\] Now note that \\(\\frac{j}{r} \\rightarrow 0\\) and \\(\\left(\\frac{1}{1+ \\mu / r}\\right)^{y} \\rightarrow 1\\) as \\(r \\rightarrow \\infty\\), so we have: \\[\n  \\lim_{r \\rightarrow \\infty} f(y; r, \\mu)\\ =\\ \\frac{\\mu^y}{y!} \\times \\lim_{r \\rightarrow \\infty} \\left(\\frac{1}{1+ \\mu / r}\\right)^{r} .\n\\] The last bit is easy once we note that: \\[\n\\begin{aligned}\n  \\lim_{r \\rightarrow \\infty} \\left(\\frac{1}{1+ \\mu / r}\\right)^{r}\\ &=\\\n      \\lim_{r \\rightarrow \\infty} \\left(1 + \\frac{\\mu}{r}\\right)^{-r}\\\\\n      &=\\ \\left[\\lim_{r \\rightarrow \\infty} \\left(1 + \\frac{\\mu}{r}\\right)^{r/\\mu}\\right]^{-\\mu}\\\\\n      &=\\ e^{-\\mu} .\n\\end{aligned}\n\\]\nPutting all of this together, we have: \\[\n  \\lim_{r \\rightarrow \\infty} f(y; r, \\mu)\\ =\\ \\frac{e^{-\\mu} \\cdot \\mu^y}{y!}.\n\\]\nFigure 1 shows the negative binomial pmf (in black) with increasing \\(r\\), and the Poisson pmf in blue. We can clearly see the negative binomial pmf resembles the poisson more and more as \\(r\\) increases. The animation was created using the excellent Julia packages Makie.jl and AlgebraOfGraphics.jl, see Danisch and Krumbiegel (2021).\n\n\n\n\n\n\nFigure 1: Negative binomial pmf (black) approaches the Poisson pmf (blue) as \\(r\\) increases.\n\n\n\n\n\nThanks to Steven Moen, Margaret Turner, and Sam Ozminkowski for suggestions and edits."
  },
  {
    "objectID": "posts/nb-pois/index.html#footnotes",
    "href": "posts/nb-pois/index.html#footnotes",
    "title": "Poisson as a Limit of the Negative Binomial Distribution",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThree views of the negative binomial distribution and Yet another view of the negative binomial distribution↩︎"
  },
  {
    "objectID": "hidden.html",
    "href": "hidden.html",
    "title": "Hidden listings page",
    "section": "",
    "text": "Faster rank(::QRPivoted) function\n\n\n\nJulia\n\nLinear Algebra\n\n\n\nBenchmarking implementations of rank functions\n\n\n\n\n\nNov 28, 2024\n\n\nAjinkya Kokandakar\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "Statistical Consulting at UW-Madison",
    "section": "",
    "text": "I work as a Project Assistant in the Statistical Consulting Group which is a part of the Department of Statistics at the University of Wisconsin–Madison. As a consultant, I have worked with more than 20 clients helping them with experimental design, data analysis and conveying statistical results. Some of our work has been highlighted in this article. If you are affiliated with UW-Madison and want statistics-related help for your research project, please visit the group’s website."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog posts",
    "section": "",
    "text": "Poisson as a Limit of the Negative Binomial Distribution\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2025\n\n4 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hidden/2024-11-28-allocations-runtime/index.html",
    "href": "hidden/2024-11-28-allocations-runtime/index.html",
    "title": "Faster rank(::QRPivoted) function",
    "section": "",
    "text": "Why am I writing about this?\nThe current implementation of the rank method for pivoted QR decomposition in julia is slow and inefficient. The modified version proposed here improves run time by orders of magnitude.\n\n\nLoad necessary packages\n\nusing LinearAlgebra\nusing Chairmarks\nusing PrettyChairmarks\nusing Statistics\nimport LinearAlgebra: rank\nusing TidierPlots\nusing DataFrames\n\nTidierPlots_set(\"plot_show\", false)\n\nfalse\n\n\n\n\nComputing the rank from a pivoted QR decomposition\nLet’s generate a \\(n \\times n\\) matrix say \\(A\\) with rank \\(r\\), by first generating a \\(n \\times r\\) matrix and then computing its outerproduct.\n\nn = 100\nr = 20\n\nX = randn(n, r)\n\noutprod(X::Matrix) = X * X'\nA = X * X'\n\nπ\n\nNow let’s compute the pivoted QR factorization of A.\n\nAqr = qr(A, ColumnNorm());\n\n\n\nGenerate matrix of known rank\nThe following function generates a \\(n \\times m\\) matrix of rank \\(r \\leq \\min(n, m)\\). We will use this for benchmarking the implementations. This function was based on a stackoverflow answer here.\n\n# Generate an `n × m` matrix of rank `r`\nfunction generatematrix(n::T, m::T, r::T) where {T&lt;:Integer}\n    A = randn(n, r)\n    B = randn(m, r)\n    return A * B'\nend\n\ngeneratematrix (generic function with 1 method)\n\n\n\n\nComputing the rank\nThe current implmentation in the julia repo is as follows:\n\nfunction rank(A::QRPivoted; atol::Real=0, rtol::Real=min(size(A)...) * eps(real(float(one(eltype(A.Q))))) * iszero(atol))\n    m = min(size(A)...)\n    m == 0 && return 0\n    tol = max(atol, rtol*abs(A.R[1,1]))\n    return something(findfirst(i -&gt; abs(A.R[i,i]) &lt;= tol, 1:m), m+1) - 1\nend\n\nrank (generic function with 16 methods)\n\n\nThe following implementation (ranfast) is orders of magnitude faster. See sections below for tests and benchmarks.\n\nfunction rankfast(A::QRPivoted; atol::Real=0, rtol::Real=min(size(A)...) * eps(real(float(one(eltype(A.Q))))) * iszero(atol))\n    m = min(size(A)...)\n    m == 0 && return 0\n    rdiag = diag(getfield(A, :factors))\n    tol = max(atol, rtol*abs(rdiag[1]))\n\n    return something(findfirst(abs.(rdiag) .&lt;= tol), m+1) - 1\nend\n\nrankfast (generic function with 1 method)\n\n\n\nrank(Aqr) == rankfast(Aqr)\n\ntrue\n\n\n\nrankbench = @bs rank($Aqr)\n\n\nChairmarks.Benchmark: 393 samples with 1 evaluation.\n Range (min … max):   82.209 μs …   5.556 ms  ┊ GC (min … max): 0.00% … 98.04%\n Time  (median):     143.250 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   200.342 μs ± 304.015 μs  ┊ GC (mean ± σ):  7.85% ± 23.13%\n\n  ▄ ▂▂█▆▅▂▂▁▁▁                                                   \n  ████████████▇▁▅▆▁▅▁▁▁▁▁▁▄▁▄▆▅▇▄█▆▅▅▅▁▆▄▁▁▄▁▁▁▁▄▁▁▁▁▁▁▄▄▁▁▁▁▁▄ ▆\n  82.2 μs       Histogram: log(frequency) by time        802 μs &lt;\n\n Memory estimate: 1.68 MiB, allocs estimate: 67.\n\n\n\n\nrankfastbench = @bs rankfast($Aqr)\n\n\nChairmarks.Benchmark: 1882 samples with 266 evaluations.\n Range (min … max):   87.876 ns …  28.528 μs  ┊ GC (min … max): 0.00% … 99.28%\n Time  (median):     100.722 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   189.035 ns ± 933.353 ns  ┊ GC (mean ± σ):  1.14% ± 10.50%\n\n  █                                                              \n  █▅▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▄ ▇\n  87.9 ns       Histogram: log(frequency) by time          5 μs &lt;\n\n Memory estimate: 1.05 KiB, allocs estimate: 6.\n\n\n\n\n\nVarying the rank\nNow we generate matrices of size \\(1000 \\times 100\\), with ranks increasing from 1 to 100, to compare the two implementations.\n\nn = 100; m = 30\n\n\nbenchs = Vector{Chairmarks.Benchmark}()\nbenchf = Vector{Chairmarks.Benchmark}()\n\n\nfor r in 1:min(n,m)\n  _qrA = qr(generatematrix(n, m, r), ColumnNorm())\n  _br = @be rank($_qrA)\n  _brf = @be rankfast($_qrA)\n\n  if (rank(_qrA) != rankfast(_qrA)) \n    println(\"uh oh rank didnt match\") \n  else\n    print(\".\")\n  end\n\n  push!(benchs, _br)\n  push!(benchf, _brf)\nend\n\n..............................\n\n\n\nmedslow = [median(bnc) for bnc in benchs]\nmedfast = [median(bnc) for bnc in benchf]\n\n\nxs = collect(1:min(m,n))\n\ndf = DataFrame(\n  rank = xs,\n  median_time = [b.time for b in medslow],\n  median_time_fast = [b.time for b in medfast],\n  medianbytes = [b.bytes for b in medslow],\n  medianbytes_fast = [b.bytes for b in medfast])\n\nggplot(df) +\n  geom_line(\n    @aes(x = rank, y = median_time), color = \"red\"\n  ) +\n  geom_line(\n    @aes(x = rank, y = median_time_fast), color = \"blue\"\n  ) +\n  labs(\n    x = \"rank\", \n    y = \"runtime (in seconds, log scale)\", \n    title = \"runtime of current (red) and faster (blue) method for $n x $m matrix\"\n  ) +\n  scale_y_log10()\n\n\n\n\n\nxs = collect(1:min(m,n))\n\nggplot(df) +\n  geom_line(\n    @aes(x = rank, y = medianbytes), color = \"red\"\n  ) +\n  geom_line(\n    @aes(x = rank, y = medianbytes_fast), color = \"blue\"\n  ) +\n  labs(x = \"rank\", \n  y = \"memory allocated (in bytes, log scale)\", title = \"Median memory allocated of current (red) and faster (blue) method for $n x $m matrix\") +\n  scale_y_log10()\n\n\n\n\n\n\nWhy is the current implementaiton so slow?\nIt’s slow because it calls Aqr.R for every diagonal entry, which leads to computing the upper triangular matrix each time! If min(n, m) is large, that can mean recreating this matrix a lot of times! The code below demonstrates this.\n\n\nA = generatematrix(10000, 100, 80);\nAqr = qr(A, ColumnNorm());\n\n\n@bs for i in 1:min(size(A)...) Aqr.R[i,i] end\n\n\nChairmarks.Benchmark: 79 samples with 1 evaluation.\n Range (min … max):  529.041 μs … 8.780 ms  ┊ GC (min … max):  0.00% … 91.60%\n Time  (median):     765.875 μs             ┊ GC (median):     0.00%\n Time  (mean ± σ):     1.243 ms ± 1.484 ms  ┊ GC (mean ± σ):  13.94% ± 28.34%\n\n  ▄█▅▁                                                         \n  ████▅▁▁██▆▆▅▁▁▅▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▅ ▁\n  529 μs       Histogram: log(frequency) by time      7.82 ms &lt;\n\n Memory estimate: 7.64 MiB, allocs estimate: 503.\n\n\n\n\n@bs for i in 1:min(size(A)...) Aqr.factors[i,i] end\n\n\nChairmarks.Benchmark: 3432 samples with 5 evaluations.\n Range (min … max):  4.725 μs …  10.300 μs  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     5.400 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   5.368 μs ± 374.269 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n                  ▄▁▁▅█▆                                       \n  ▂▄▅▆▃▃▂▁▁▁▃▅▄▃▂▃██████▇▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁ ▂\n  4.73 μs         Histogram: frequency by time        6.84 μs &lt;\n\n Memory estimate: 4.77 KiB, allocs estimate: 203.\n\n\n\n\n@bs Aqr.R[1,1]\n\n\nChairmarks.Benchmark: 7761 samples with 1 evaluation.\n Range (min … max):   3.791 μs …  2.987 ms  ┊ GC (min … max): 0.00% … 98.99%\n Time  (median):      7.750 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   10.881 μs ± 69.273 μs  ┊ GC (mean ± σ):  0.20% ±  4.44%\n\n  ▃ ▅     ▁▆█▆▇▇▄▄▂                                            \n  █▇█▇▃▄▂▃██████████▇▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▂▂▁▂▁▂▂▂▂ ▄\n  3.79 μs         Histogram: frequency by time          23 μs &lt;\n\n Memory estimate: 78.33 KiB, allocs estimate: 8.\n\n\n\n\n@bs Aqr.factors[1,1]\n\n\nChairmarks.Benchmark: 3022 samples with 988 evaluations.\n Range (min … max):  27.117 ns …  4.323 μs  ┊ GC (min … max): 0.00% … 98.66%\n Time  (median):     29.774 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   31.338 ns ± 78.120 ns  ┊ GC (mean ± σ):  0.03% ±  1.79%\n\n   ▂▅▅▄▄▃▂▁ ▁▄▆██▇▇▅▄▃▂▂                                      ▁\n  ███████████████████████▇▇▇█▆██▇██▆█▆▇█▇▇▇▄▆▅▅▅▄▃▅▃▄▄▄▄▅▄▅▄▅ █\n  27.1 ns      Histogram: log(frequency) by time        38 ns &lt;\n\n Memory estimate: 16.11336032388664 bytes, allocs estimate: 1.\n\n\n\n\n\nAlloc can get bad for large matrices!\nIf the matrix is large (even if the rank is small), the current method allocates too much! See example below.\n\nMqr = qr(generatematrix(1000, 100, 50), ColumnNorm());\n\nThe current implementation allocated ~400 megabytes and takes on the order of 100 ms to run!\n\n@bs rank($Mqr)\n\n\nChairmarks.Benchmark: 199 samples with 1 evaluation.\n Range (min … max):  215.125 μs …   3.596 ms  ┊ GC (min … max): 0.00% … 92.12%\n Time  (median):     348.416 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   438.085 μs ± 340.348 μs  ┊ GC (mean ± σ):  7.49% ± 22.52%\n\n  ▁▁ ▃▄▇█▇▆▃▁                                                    \n  ██▇████████▅▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▅▅▅▁▁▁▅▁▁▅▆▆▅▅▆▅▁▅▁▁▁▁▅▁▅ ▅\n  215 μs        Histogram: log(frequency) by time       1.43 ms &lt;\n\n Memory estimate: 3.97 MiB, allocs estimate: 157.\n\n\n\nThe faster method only allocates ~10 kilobytes and the runtime is in μs!!\n\n@bs rankfast($Mqr)\n\n\nChairmarks.Benchmark: 3835 samples with 150 evaluations.\n Range (min … max):   87.500 ns … 41.346 μs  ┊ GC (min … max): 0.00% … 99.48%\n Time  (median):     101.947 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   159.042 ns ±  1.090 μs  ┊ GC (mean ± σ):  0.26% ±  5.05%\n\n         ▃▃█▅▇▃▁                                                \n  ▂▃▃▅▅▆▆███████▆▅▃▃▂▂▃▂▃▂▃▃▅▄▆▅▅▅▄▅▃▃▃▃▂▃▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▂▂▂ ▃\n  87.5 ns         Histogram: frequency by time          163 ns &lt;\n\n Memory estimate: 1.05 KiB, allocs estimate: 6.\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{kokandakar2024,\n  author = {Kokandakar, Ajinkya},\n  title = {Faster {`rank(::QRPivoted)`} Function},\n  date = {2024-11-28},\n  url = {https://ajinkya-k.github.io/posts/2024-18-24-allocations-runtime/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKokandakar, Ajinkya. 2024. “Faster `Rank(::QRPivoted)`\nFunction.” November 28, 2024. https://ajinkya-k.github.io/posts/2024-18-24-allocations-runtime/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ajinkya Kokandakar",
    "section": "",
    "text": "CV\n  \n  \n    \n     GitHub\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Google Scholar\n  \n  \n     arXiv\n  \n  \n     ORCiD\n  \n\n  \n  \n\n\nPh.D. Candidate in Statistics | Statistical Consultant The University of Wisconsin–Madison ajinkya@stat.wisc.edu\n\nResearch: I am a Ph.D. candidate in Statistics at the UW-Madison advised by Dr. Sameer Deshpande, pursuing research in causal inference and Bayesian methods. Currently, my work focuses on heterogeneous treatment effect estimation, and causal inference when the exposure is not very well defined.\nConsulting: I work as a consultant in the Statistical Consulting Group at UW-Madison (formerly the CALS Consulting Lab), assisting clients from life sciences with experimental design and data analysis. Some of our work has been highlighted in this article.\nOpen Source Code: I have experience writing software packages and modules in R/C++. I also occasionally contribute to open source projects especially in the Julia and R ecosystems.\nBackground: I worked as a Data Scientist intern at Mathematica Inc from June - Dec 2024. Prior to joining UW—Madison, I graduated with a MS in Economics and Computation at Duke University (2020). I got my undergraduate dual-degree in Computer Science and Economics from Birla Institute of Technology and Science, Pilani and worked as a Software Engineer at Infosys Ltd. in India"
  },
  {
    "objectID": "index.html#ajinkya-kokandakar",
    "href": "index.html#ajinkya-kokandakar",
    "title": "Ajinkya Kokandakar",
    "section": "",
    "text": "Ph.D. Candidate in Statistics | Statistical Consultant The University of Wisconsin–Madison ajinkya@stat.wisc.edu\n\nResearch: I am a Ph.D. candidate in Statistics at the UW-Madison advised by Dr. Sameer Deshpande, pursuing research in causal inference and Bayesian methods. Currently, my work focuses on heterogeneous treatment effect estimation, and causal inference when the exposure is not very well defined.\nConsulting: I work as a consultant in the Statistical Consulting Group at UW-Madison (formerly the CALS Consulting Lab), assisting clients from life sciences with experimental design and data analysis. Some of our work has been highlighted in this article.\nOpen Source Code: I have experience writing software packages and modules in R/C++. I also occasionally contribute to open source projects especially in the Julia and R ecosystems.\nBackground: I worked as a Data Scientist intern at Mathematica Inc from June - Dec 2024. Prior to joining UW—Madison, I graduated with a MS in Economics and Computation at Duke University (2020). I got my undergraduate dual-degree in Computer Science and Economics from Birla Institute of Technology and Science, Pilani and worked as a Software Engineer at Infosys Ltd. in India"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Ajinkya Kokandakar",
    "section": "Publications and Pre-prints",
    "text": "Publications and Pre-prints\nDouglas Bates, Phillip M. Alday, and Ajinkya H. Kokandakar Mixed-model Log-likelihood Evaluation Via a Blocked Cholesky Factorization (2025)   arXiv:2505.11674 [preprint] [replication material]\n\nAjinkya H. Kokandakar, Yuzhou Lin, Steven Jin, Jordan Weiss, Amanda R. Rabinowitz, Reuben A. Buford May, Dylan Small, Sameer K. Deshpande Adolescent sports participation and health in early adulthood: An observational study (2025) Youth & Society  [paper] [preprint] [code]\n Eliot J. Kim, Tracey Holloway, Ajinkya H. Kokandakar, Monica Harkey, Stephanie Elkins, Daniel L. Goldberg, Colleen Heck, (2024) A Comparison of Regression Methods for Inferring Near-Surface NO2 with Satellite Data  Journal of Geophysical Research: Atmospheres, 129, e2024JD040906  [paper] [preprint]\n Ajinkya H. Kokandakar, Yuzhou Lin, Steven Jin, Jordan Weiss, Amanda R. Rabinowitz, Reuben A. Buford May, Sameer K. Deshpande, Dylan Small, (2024). “Pre-analysis protocol for an observational study on the effects of adolescent sports participation on health in early adulthood.” Observational Studies 10(1), 11-35  [paper] [preprint] [code]\n Ajinkya H. Kokandakar, Hyunseung Kang, Sameer K. Deshpande, (2023). “Bayesian causal forests and the 2022 ACIC Data Challenge: scalability and sensitivity.” Observational Studies, 9(3), 29-41. [paper] [preprint] [code]\n Jagat Sesh Challa, Poonam Goyal, Ajinkya Kokandakar, Dhananjay Mantri, Pranet Verma, Sundar Balasubramaniam & Navneet Goyal (2022). “Anytime clustering of data streams while handling noise and concept drift.” Journal of Experimental & Theoretical Artificial Intelligence, 34(3), 399-429.  [paper]"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "Ajinkya Kokandakar",
    "section": "Work Experience",
    "text": "Work Experience\nMathematica Inc. Data Science Intern (June - Aug 2024 full-time, Aug - Dec 2024 part-time)\n\nDeveloping and using Bayesian non-parametric methods for subgroup analysis in a large scale healthcare impact evaluation\n\nUniversity of Wisconsin – Madison Project Assistant, Statistical Consulting Group (Aug 2023 - May 2024)\n\nAssisted more than 15 clients (graduate students and postdocs) from the College of Agriculture and Life Sciences with experimental design and analysis data obtained from experiments\n\nInfosys Ltd  Specialist Programmer (July 2017 - May 2018)\n\nDesigned and developed the telemetry and data analytics module for the company’s internal learning platform"
  },
  {
    "objectID": "index.html#research-experience",
    "href": "index.html#research-experience",
    "title": "Ajinkya Kokandakar",
    "section": "Research Experience",
    "text": "Research Experience\nUniversity of Wisconsin – Madison Research Assistant\n\nDepartment of Statistics, Advisors: Prof Sameer Deshpande and Prof Keith Levin (May 2022 – Aug 2022)\nDepartment of Biostatistics and Medical Informatics, Advisors: Prof Menggang Yu and Prof Guanhua Chen (June 2020 – Dec 2021)\n\nDuke University Research Assistant\n\nThe Fuqua School of Business Advisor: Prof Giuseppe Lopomo (June 2019 - May 2020)\nDepartment of Economics Advisor: Prof Matt Masten (June 2019 - May 2020)\nDepartment of Economics Advisor: Prof Arjada Bardhi (Jan 2019 - May 2019)\n\nBirla Institute of Technology and Science, Pilani\n\nUndergraduate Scholar, Advisor: Prof S. Balasubramaniam, ADAPT Lab, BITS Pilani (Jan 2017 - Dec 2017)"
  },
  {
    "objectID": "index.html#teaching-experience",
    "href": "index.html#teaching-experience",
    "title": "Ajinkya Kokandakar",
    "section": "Teaching Experience",
    "text": "Teaching Experience\nUniversity of Wisconsin – Madison Teaching Assistant, Department of Statistics\n\nSTAT 451: Introduction to Machine Learning and Statistical Pattern Classification (Fall 2023)\nSTAT 240: Data Science Modeling I (Fall 2022)\nSTAT 371: Introductory Applied Statistics for the Life Sciences (Spring 2022)\n\nDuke University Teaching Assistant, Department of Computer Science\n\nCOMPSCI 370: Introduction to Artificial Intelligence (Spring 2020)\nCOMPSCI 201: Algorithms and Data Structures (Spring 2019)\n\nBirla Institute of Technology and Science, Pilani Undergraduate Teaching Assistant\n\nECON F211: Principles of Economics\nECON F212: Fundamentals of Finance and Accounting\nECON F412: Securities Analysis and Portfolio Management\nCS F211: Data Structures and Algorithms"
  },
  {
    "objectID": "opensource.html",
    "href": "opensource.html",
    "title": "Open Source Contributions",
    "section": "",
    "text": "In addition to writing software for my own research, I regularly contribute to open source projects related to statistics, especially in the Julia programming language ecosystem. So far I have contributed to the following packages:\n\nJulia language documentation\nLinearAlgebra.jl\nStatistics.jl\nMixedModels.jl\nGLM.jl\nPrettyChairmarks.jl\n\nI have also contributed to the R language documentation and submitted bug-fixes to other R packages (see 1, 2).\nAll GitHub PRs here."
  }
]